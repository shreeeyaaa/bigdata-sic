{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Big Data Computing","text":"<p>Samsung Innovation Campus (SIC)</p>"},{"location":"#introduction","title":"Introduction","text":"<p>As part of the Big Data Computing course offered by SIC, we conducted a series of hands-on laboratory sessions aimed at understanding and applying core concepts of the Hadoop ecosystem. These labs provided practical exposure to key components such as:</p> <ul> <li>HDFS (Hadoop Distributed File System)  </li> <li>YARN (Yet Another Resource Negotiator)  </li> <li>MapReduce </li> <li>Big Data Ingestion using Sqoop</li> </ul> <p>The exercises focused on both theoretical understanding and technical implementation, helping us build foundational skills required for working with large-scale distributed systems.</p>"},{"location":"#lab-overview","title":"Lab Overview","text":"<p>During the course, we successfully completed the following lab exercises:</p>"},{"location":"#lab-2-working-with-hdfs","title":"Lab 2: Working with HDFS","text":"<p>We explored the Hadoop Distributed File System (HDFS), learned how to store and retrieve data in a distributed environment, and practiced basic file operations such as:</p> <ul> <li>Creating directories</li> <li>Uploading files</li> <li>Reading files</li> <li>Deleting data in HDFS</li> </ul>"},{"location":"#lab-31-working-with-yarnmapreduce","title":"Lab 3.1: Working with YARN/MapReduce","text":"<p>In this lab, we executed a classic WordCount MapReduce job using the YARN resource management framework. We monitored job execution through the YARN Resource Manager Web UI and analyzed the outputs stored in HDFS.</p>"},{"location":"#lab-32-big-data-ingestion","title":"Lab 3.2: Big Data Ingestion","text":"<p>This lab focused on data ingestion using Apache Sqoop. We practiced:</p> <ul> <li>Importing data from relational databases (MariaDB) into HDFS</li> <li>Working with different file formats (Text, Parquet)</li> <li>Applying compression techniques (Snappy)</li> <li>Selectively importing specific columns or filtered rows</li> <li>Exporting data back to the database</li> </ul> <p>Each of these labs involved step-by-step execution of commands, verification of outputs, and troubleshooting any errors encountered during the process.</p>"},{"location":"#lab-environment-and-preparation","title":"Lab Environment and Preparation","text":"<p>The lab exercises were performed in a virtualized environment where the Hadoop ecosystem was pre-installed.</p> <p>Before beginning each session, we ensured that:</p> <ul> <li>All Hadoop services (NameNode, DataNode, ResourceManager, NodeManager, etc.) were running properly.</li> <li>We were familiar with basic Linux commands and terminal navigation, as these skills were essential for interacting with the Hadoop framework.</li> </ul>"},{"location":"lab2-hdfs/","title":"Lab 2: Working with HDFS","text":""},{"location":"lab2-hdfs/#objective","title":"Objective","text":"<p>The objective of this lab is to gain hands-on experience with the Hadoop Distributed File System (HDFS). We learned to start Hadoop services, create and manage users and directories in both Linux and HDFS, transfer files into HDFS, and explore HDFS contents via both the command line and the Web UI.</p>"},{"location":"lab2-hdfs/#1-starting-hdfs-and-yarn-services","title":"1. Starting HDFS and YARN Services","text":"<p>To begin the lab, we launched the required Hadoop daemons. This step was critical to initiate the core services that allow HDFS and YARN to function.</p> <p>We opened a terminal and switched to the <code>hadoop</code> user using:</p> <pre><code>su - hadoop\n</code></pre> <p>Then, we navigated to the <code>sbin</code> directory and executed the following scripts:</p> <p><pre><code>cd ~/hadoop/sbin\n./start-dfs.sh\n./start-yarn.sh\n</code></pre> </p> <p>This started the NameNode, DataNode (for HDFS), and the ResourceManager and NodeManager (for YARN). Successful output messages confirmed the services were up and running.</p>"},{"location":"lab2-hdfs/#2-exploring-hdfs-and-linux-file-systems","title":"2. Exploring HDFS and Linux File Systems","text":"<p>Next, we logged in as student and explored the structure of HDFS and local Linux file systems. As the <code>student</code> user, we listed the root directory of HDFS:</p> <p><pre><code>hdfs dfs -ls /\n</code></pre> </p> <p>This displayed top-level directories such as <code>/tmp</code> and <code>/user</code>. We then checked for the presence of a home directory for the <code>student</code> user:</p> <p><pre><code>hdfs dfs -ls /user\nhdfs dfs -ls /user/student\n</code></pre> </p> <p>It was empty initially. In parallel, we examined the local Linux home directory:</p> <p><pre><code>cd /home/student\nls -l\n</code></pre> </p> <p>This step highlighted the dual file systems in use\u2014HDFS and Linux.</p>"},{"location":"lab2-hdfs/#3-creating-a-new-linux-and-hdfs-user","title":"3. Creating a New Linux and HDFS User","text":"<p>We then performed user administration tasks by creating a new Linux user (<code>student2</code>) and a corresponding HDFS home directory.</p> <p>To add a new user with superuser privileges:</p> <pre><code>sudo useradd student2 -m\nsudo passwd student2\nsudo usermod -aG wheel student2\n</code></pre> <p>We tested the new user:</p> <p><pre><code>su - student2\ncd\npwd\ngroups\n</code></pre> </p> <p>For HDFS, as <code>hadoop</code> user, we created and assigned ownership of a new directory:</p> <pre><code>hdfs dfs -mkdir /user/student2\nhdfs dfs -chown student2 /user/student2\n</code></pre> <p></p> <p>After switching to the student2 user account, we verified the creation of the user's HDFS home directory by running the following command in the terminal: <pre><code>hdfs dfs -ls /user/student2\n</code></pre></p> <p>Although no files or subdirectories were listed (since /user/student2 was empty), the absence of an error message confirmed that the directory exists in HDFS. This verified that the home directory for the student2 user had been successfully created.</p> <p>To further confirm the behavior of HDFS when a directory does not exist, we attempted to list the contents of a non-existent directory by running: <pre><code>hdfs dfs -ls /user/student3\n</code></pre></p> <p></p>"},{"location":"lab2-hdfs/#4-file-handling-in-hdfs","title":"4. File Handling in HDFS","text":"<p>Returning to the <code>student</code> user, we created a subdirectory in HDFS for a MapReduce test:</p> <p><pre><code>hdfs dfs -mkdir MRtest\n</code></pre> </p> <p>We navigated to the local file containing Alice in Wonderland:</p> <p><pre><code>cd ~/Data\nless alice_in_wonderland.txt\n</code></pre> </p> <p>Then, we uploaded this file into our HDFS MRtest directory:</p> <pre><code>hdfs dfs -put ~/Data/alice_in_wonderland.txt /user/student/MRtest/\n</code></pre> <p>To ensure the file was successfully transferred, we listed the contents:</p> <p><pre><code>hdfs dfs -ls /user/student/MRtest\n</code></pre> </p>"},{"location":"lab2-hdfs/#5-exploring-hdfs-with-the-web-ui","title":"5. Exploring HDFS with the Web UI","text":"<p>To visually inspect HDFS, we opened the Firefox browser and visited:</p> <pre><code>http://localhost:9870\n</code></pre> <p>We examined the following sections:</p> <ul> <li>Overview Tab: Displayed summary of datanodes and disk capacity.</li> <li>Datanodes: Showed health and activity of nodes (only one in our case).</li> <li>Snapshot: Presented snapshot options for directories.</li> <li>Startup Progress: Detailed the loading of fsimage and edits file.</li> </ul> <p>From the Utilities &gt; Browse the file system, we navigated to <code>/user/student/MRtest</code> and opened <code>alice_in_wonderland.txt</code>. We viewed the Block Information, which included the Block ID and Block Pool ID.</p> <p></p>"},{"location":"lab2-hdfs/#6-locating-block-storage-on-disk","title":"6. Locating Block Storage on Disk","text":"<p>We used the terminal to locate the actual block files on the disk using the <code>find</code> command. First, as <code>hadoop</code> user, we searched for the Block Pool ID:</p> <pre><code>sudo find / -name &lt;BlockPoolID&gt; -print\n</code></pre> <p>After locating the directory, we searched for files related to our specific Block ID:</p> <p><pre><code>cd &lt;path_found&gt;\nsudo find . -name *&lt;BlockID&gt;* -print\n</code></pre> </p> <p>The block file and its metadata were discovered. We viewed the actual content of the text block using:</p> <p><pre><code>cat &lt;block_file_path&gt;\n</code></pre> </p> <p>This confirmed that Hadoop had stored our text file successfully in its block-based architecture.</p>"},{"location":"lab2-hdfs/#summary","title":"Summary","text":"<p>In Lab 2, we:</p> <ul> <li>Started and validated HDFS and YARN services.</li> <li>Explored and differentiated between Linux and HDFS directories.</li> <li>Created new users in both systems.</li> <li>Uploaded and verified files in HDFS.</li> <li>Explored HDFS using the Web UI.</li> <li>Located physical block files on disk, reinforcing Hadoop\u2019s data storage mechanism.</li> </ul> <p>This lab provided essential understanding and practical experience with HDFS fundamentals, file operations, user management, and internal storage architecture.</p>"},{"location":"lab3/","title":"Lab 3.1: Working with YARN and MapReduce","text":""},{"location":"lab3/#objective","title":"Objective","text":"<p>The goal of Lab 3 is to understand how YARN and MapReduce work in a Hadoop environment. We utilized a sample file already stored in HDFS (<code>alice_in_wonderland.txt</code>) to perform a word count using MapReduce. We also monitored job execution using the YARN Resource Manager Web UI.</p>"},{"location":"lab3/#1-starting-the-history-server","title":"1. Starting the History Server","text":"<p>To enable job tracking via the web interface, we needed to start the YARN history server. This was done as the <code>hadoop</code> user:</p> <pre><code>su - hadoop\ncd $HADOOP_HOME/sbin\nmr-jobhistory-daemon.sh start historyserver\n</code></pre> <p></p> <p>This warning indicates that the mr-jobhistory-daemon.sh script is deprecated in newer versions of Hadoop. The system automatically attempted to use the newer command format instead.</p> <p>Successful execution confirmed the history server had been initiated. This service allows us to monitor past and running MapReduce jobs from a web browser.</p>"},{"location":"lab3/#2-running-the-wordcount-mapreduce-program","title":"2. Running the WordCount MapReduce Program","text":"<p>With the services running, we switched to the <code>student</code> user and navigated to the directory containing the Hadoop examples JAR:</p> <pre><code>su - student\ncd $HADOOP_HOME/share/hadoop/mapreduce\n</code></pre> <p>We executed the wordcount MapReduce job using:</p> <pre><code>hadoop jar ./hadoop-mapreduce-examples-3.3.1.jar wordcount MRtest WC_Output\n</code></pre> <p>Here:</p> <ul> <li><code>MRtest</code> is the input directory in HDFS containing <code>alice_in_wonderland.txt</code></li> <li><code>WC_Output</code> is the output directory where the word count results will be stored</li> </ul> <p>During execution, we could see the job being processed in the terminal, including Map and Reduce progress bars.</p> <p></p>"},{"location":"lab3/#3-monitoring-job-on-yarn-web-ui","title":"3. Monitoring Job on YARN Web UI","text":"<p>While the word count job was running, we opened a web browser and navigated to:</p> <pre><code>http://localhost:8088\n</code></pre> <p>On this YARN Resource Manager interface, we did the following:</p> <ul> <li>Clicked on the \"Applications\" tab to see active and completed jobs.</li> <li>Located the job using its Application ID.</li> <li>Clicked on the Application ID to view detailed execution logs, counters, and metrics.</li> <li>Navigated to the Application Master UI, which provided container-level resource usage details.</li> </ul> <p>These insights helped us better understand how YARN allocates and manages resources for a job.</p> <p> </p>"},{"location":"lab3/#4-viewing-output-in-hdfs","title":"4. Viewing Output in HDFS","text":"<p>After job completion, we verified the output directory and results:</p> <p><pre><code>hdfs dfs -ls WC_Output\nhdfs dfs -cat WC_Output/part-r-00000\n</code></pre> </p> <p>This showed us the word frequency results generated by the MapReduce job, with each line representing a word and its count.</p> <p></p> <p>To rerun the job or perform new ones using the same output directory name, we needed to delete the existing output directory first:</p> <pre><code>hdfs dfs -rm -r WC_Output\n</code></pre> <p>This command recursively deleted the <code>WC_Output</code> directory in HDFS.</p>"},{"location":"lab3/#summary","title":"Summary","text":"<p>In Lab 3, we:</p> <ul> <li>Started the MapReduce job history server</li> <li>Executed a word count MapReduce program on a text file in HDFS</li> <li>Monitored the job in real-time using the YARN Web UI</li> <li>Viewed and analyzed the word count results in HDFS</li> </ul> <p>This lab demonstrated the flow of a typical Hadoop MapReduce job from initiation to completion, emphasizing the roles of YARN, Resource Manager, and the MapReduce programming model.</p>"},{"location":"lab31/","title":"Lab 3.2: Data Ingestion with Sqoop for RDBMS (MariaDB)","text":""},{"location":"lab31/#objective","title":"Objective","text":"<p>The goal of this lab is to understand how to ingest data from relational databases into Hadoop Distributed File System (HDFS) using Sqoop. We performed various Sqoop operations such as importing entire tables, selective columns, filtering rows, using different file formats, and exporting data back to RDBMS.</p>"},{"location":"lab31/#1-exploring-the-database-mariadb","title":"1. Exploring the Database (MariaDB)","text":"<p>We began by accessing the MariaDB database and inspecting its structure:</p> <pre><code>mysql --user=student --password=student labs\n</code></pre> <p>We checked the existing databases and tables using:</p> <p><pre><code>show databases;\nshow tables;\ndescribe authors;\ndescribe posts;\n</code></pre> </p> <p>This showed us the tables <code>authors</code> and <code>posts</code>, which we are going to use for Sqoop operations.</p> <p>To understand the structure and data inside the tables, we ran the following SQL query to view some sample records from the <code>authors</code> table:</p> <p><pre><code>SELECT id, first_name, last_name, email, added\nFROM authors\nLIMIT 5;\n</code></pre> </p> <p>This command displayed the first five rows of the <code>authors</code> table, including useful fields like the author's ID, name, email, and the date they were added. This step helps verify that the data exists and is in the expected format before performing any Sqoop imports.</p> <p>After completing the exploration, we exited the MariaDB shell by typing:</p> <pre><code>quit\n</code></pre>"},{"location":"lab31/#2-sqoop-basic-commands","title":"2. Sqoop Basic Commands","text":"<p>We checked the basic Sqoop commands and help menu:</p> <p><pre><code>sqoop help\nsqoop help import\n</code></pre>  We also listed the databases and tables in MariaDB using:</p> <p><pre><code>sqoop list-databases --connect jdbc:mysql://localhost --username student --password student\nor\nsqoop list-tables --connect jdbc:mysql://localhost/labs --username student -P\n</code></pre> </p> <p>Instead of typing the database password directly in the command using the --password option, we can use the -P (uppercase) flag. This way, Sqoop will securely prompt us to enter the password at runtime, and it won\u2019t be visible on the screen or saved in our command history.</p>"},{"location":"lab31/#3-importing-tables-into-hdfs","title":"3. Importing Tables into HDFS","text":"<p>Sqoop provides an import-all-tables command that allows you to import all tables from a database into HDFS in one step. We used the following command to import all tables from the labs database:</p> <pre><code>sqoop import-all-tables --connect jdbc:mysql://localhost/labs \\\n--username student --password student\n</code></pre> <p>This command automatically imports every table from the specified database and saves each table's data into a separate directory under the user's home directory in HDFS.</p> <p>In real environments, the import-all-tables command is rarely used because databases usually have many tables, and importing all of them at once can be time-consuming and resource-intensive. Instead, we typically import tables one by one using the import command for better control and efficiency.</p> <p>We imported the entire <code>posts</code> table into HDFS:</p> <pre><code>sqoop import --connect jdbc:mysql://localhost/labs \\\n--username student --password student --table posts\n</code></pre> <p>To check:</p> <p><pre><code>hdfs dfs -ls /user/student/posts\n</code></pre> </p>"},{"location":"lab31/#4-importing-tables-to-custom-directory-with-delimiter","title":"4. Importing Tables to Custom Directory with Delimiter","text":"<p>We created a custom HDFS directory:</p> <pre><code>hdfs dfs -mkdir /mywarehouse\n</code></pre> <p>And imported the <code>authors</code> table with comma-separated fields:</p> <pre><code>sqoop import --connect jdbc:mysql://localhost/labs \\\n--username student --password student \\\n--table authors --fields-terminated-by ',' \\\n--target-dir /mywarehouse/authors\n</code></pre> <p>To verify:</p> <p><pre><code>hdfs dfs -ls /mywarehouse/authors\nhdfs dfs -cat /mywarehouse/authors/part-m-00000\n</code></pre> </p> <p>When we executed the cat command to view the contents of the imported authors table, we observed that each line of data was separated by commas (,) as specified by the --fields-terminated-by ',' option. This is different from the earlier posts file, which used the default tab (\\t) delimiter in HDFS.</p>"},{"location":"lab31/#5-importing-selected-columns","title":"5. Importing Selected Columns","text":"<p>We imported specific columns from the <code>authors</code> table:</p> <p><pre><code>sqoop import --connect jdbc:mysql://localhost/labs \\\n--username student --password student \\\n--table authors --fields-terminated-by '\\t' \\\n--columns \"first_name, last_name, email\"\n</code></pre> </p>"},{"location":"lab31/#6-importing-with-filters","title":"6. Importing with Filters","text":"<p>We imported rows from <code>authors</code> where <code>first_name = 'Dorthy'</code>:</p> <pre><code>sqoop import --connect jdbc:mysql://localhost/labs \\\n--username student --password student \\\n--table authors --fields-terminated-by '\\t' \\\n--where \"first_name='Dorthy'\" \\\n--target-dir authors_Dorthy\n</code></pre> <p></p>"},{"location":"lab31/#7-importing-in-parquet-format","title":"7. Importing in Parquet Format","text":"<p>We imported the <code>authors</code> table as Parquet:</p> <pre><code>sqoop import --connect jdbc:mysql://localhost/labs \\\n--username student --password student \\\n--table authors --target-dir /mywarehouse/authors_parquet \\\n--as-parquetfile\n</code></pre> <p>We retrieved the Parquet file locally and viewed it using <code>parquet-tools</code>:</p> <p><pre><code>hdfs dfs -get /mywarehouse/authors_parquet/&lt;file&gt;.parquet\nparquet-tools show &lt;file&gt;.parquet\n</code></pre> </p>"},{"location":"lab31/#8-importing-with-compression","title":"8. Importing with Compression","text":"<p>We imported data with compression enabled:</p> <p><pre><code>sqoop import --connect jdbc:mysql://localhost/labs \\\n--username student --password student \\\n--table authors --target-dir /mywarehouse/authors_compressed --compress\n</code></pre> </p>"},{"location":"lab31/#9-exporting-data-from-hdfs-to-rdbms","title":"9. Exporting Data from HDFS to RDBMS","text":"<p>We exported the previously filtered data (<code>Dorthy</code>) back to MariaDB:</p> <p><pre><code>sqoop export --connect jdbc:mysql://localhost/labs \\\n--username student --password student \\\n--table authors_export --fields-terminated-by '\\t' \\\n--export-dir dorthy\n</code></pre> </p>"},{"location":"lab31/#10-advanced-exercises","title":"10. Advanced Exercises","text":"<p>We also worked with Sqoop to perform advanced data import operations into HDFS using both text and Parquet formats with compression.</p> <p>Task 1:</p> <p>To find the column names of the posts table without accessing MariaDB directly, we used the sqoop eval command. This allowed us to execute a SQL query from the terminal using Sqoop. We ran the following command to describe the structure of the posts table and retrieve its column names:</p> <pre><code>sqoop eval --connect jdbc:mysql://localhost/labs \\\n--username student --password student --query \"DESCRIBE posts;\"\n</code></pre> <p></p> <p>By running this command, we were able to view the list of columns (id, author_id, title, description, content, date) along with their data types and other information, which helped us select the appropriate columns for the subsequent data import task.</p> <p>Then we imported selected columns from the <code>posts</code> table into the HDFS directory <code>/tmp/mylabs/posts_info</code>. We chose only the <code>id</code>, <code>title</code>, and <code>date</code> columns and saved the data in text format with tab delimiters. This was done using the following command:</p> <pre><code>sqoop import --connect jdbc:mysql://localhost/labs \\\n--username student --password student \\\n--table posts --fields-terminated-by '\\t' \\\n--columns \"id, title, date\" \\\n--target-dir /tmp/mylabs/posts_info\n</code></pre> <p>For viewing: <pre><code>hdfs dfs -ls /tmp/mylabs/posts_info\nhdfs dfs -cat /tmp/mylabs/posts_info/part-m-00000\n</code></pre></p> <p></p> <p>Task 2:</p> <p>Next, we imported the same selected data into Parquet format with Snappy compression for efficient storage. We saved this data into the <code>/tmp/mylabs/posts_compressed</code> directory using the command:</p> <pre><code>sqoop import --connect jdbc:mysql://localhost/labs \\\n--username student --password student \\\n--table posts --columns \"id, title, created_at\" \\\n--target-dir /tmp/mylabs/posts_compressed \\\n--as-parquetfile --compression-codec snappy\n</code></pre> <p>For viewing: <pre><code>hdfs dfs -get /tmp/mylabs/posts_compressed/&lt;filename&gt;\nparquet-tools show &lt;filename&gt;\n</code></pre></p> <p></p> <p>Task 4:</p> <p>For this task, we imported specific columns (id, first_name, last_name, and birthdate) from the authors table into the HDFS home directory. Before starting the import, we checked for the existence of the authors directory in HDFS. Since the directory existed, we deleted it to avoid any conflicts or overwrite issues using the command:</p> <pre><code>hdfs dfs -rm -r authors\n</code></pre> <p>After clearing the directory, we executed the Sqoop import command to bring in only the specified columns. The data was saved in plain text format with tab delimiters as required:</p> <pre><code>sqoop import --connect jdbc:mysql://localhost/labs \\\n--username student --password student \\\n--table authors --fields-terminated-by '\\t' \\\n--columns \"id, first_name, last_name, birthdate\"\n</code></pre> <p></p> <p>Task 5:</p> <p>Finally, we imported rows from the <code>posts</code> table where the <code>title</code> column is not null. We selected only the <code>id</code>, <code>title</code>, and <code>content</code> columns and saved the data in Parquet format with Snappy compression into the <code>/tmp/mylabs/posts_NotN</code> directory. The command used was:</p> <p><pre><code>sqoop import --connect jdbc:mysql://localhost/labs \\\n--username student --password student \\\n--table posts --where \"title is not null\" \\\n--columns \"id, title, content\" \\\n--target-dir /tmp/mylabs/posts_NotN \\\n--as-parquetfile --compression-codec snappy\n</code></pre> </p> <p>Through these exercises, we gained hands-on experience in selectively importing data, working with different file formats, applying compression, and using conditional queries within Sqoop.</p>"},{"location":"lab31/#summary","title":"Summary","text":"<p>In this lab, we:</p> <ul> <li>Explored MariaDB databases and tables</li> <li> <p>Performed Sqoop imports with various configurations:</p> </li> <li> <p>Full table import</p> </li> <li>Selected columns import</li> <li>Conditional imports using <code>--where</code></li> <li>Text and Parquet file formats</li> <li>Compression using Snappy</li> <li>Exported HDFS data back to RDBMS</li> </ul> <p>This lab provided practical exposure to data ingestion pipelines using Sqoop, a key component of Big Data ecosystems.</p>"}]}