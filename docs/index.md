# Big Data Computing  
<!-- *Big Data Computing Course*   -->
*Samsung Innovation Campus (SIC)*

## Introduction

As part of the **Big Data Computing** course offered by **SIC**, we conducted a series of hands-on laboratory sessions aimed at understanding and applying core concepts of the **Hadoop ecosystem**. These labs provided practical exposure to key components such as:

- **HDFS** (Hadoop Distributed File System)  
- **YARN** (Yet Another Resource Negotiator)  
- **MapReduce**  
- **Big Data Ingestion using Sqoop**

The exercises focused on both theoretical understanding and technical implementation, helping us build foundational skills required for working with large-scale distributed systems.

---

## Lab Overview

During the course, we successfully completed the following lab exercises:

### Lab 2: Working with HDFS

We explored the **Hadoop Distributed File System (HDFS)**, learned how to store and retrieve data in a distributed environment, and practiced basic file operations such as:

- Creating directories
- Uploading files
- Reading files
- Deleting data in HDFS

---

### Lab 3.1: Working with YARN/MapReduce

In this lab, we executed a classic **WordCount MapReduce** job using the **YARN** resource management framework. We monitored job execution through the **YARN Resource Manager Web UI** and analyzed the outputs stored in HDFS.

---

### Lab 3.2: Big Data Ingestion

This lab focused on data ingestion using **Apache Sqoop**. We practiced:

- Importing data from relational databases (**MariaDB**) into HDFS
- Working with different file formats (Text, Parquet)
- Applying compression techniques (Snappy)
- Selectively importing specific columns or filtered rows
- Exporting data back to the database

Each of these labs involved step-by-step execution of commands, verification of outputs, and troubleshooting any errors encountered during the process.

---

## Lab Environment and Preparation

The lab exercises were performed in a **virtualized environment** where the **Hadoop ecosystem** was pre-installed.

Before beginning each session, we ensured that:

- All **Hadoop services** (*NameNode, DataNode, ResourceManager, NodeManager*, etc.) were running properly.
- We were familiar with **basic Linux commands** and **terminal navigation**, as these skills were essential for interacting with the Hadoop framework.

---
